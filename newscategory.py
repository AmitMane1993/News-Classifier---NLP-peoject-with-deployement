# -*- coding: utf-8 -*-
"""NewsCategory.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l6adcHoR2assS6WV7TsylwNq1TbnJP9i
"""

import tensorflow as tf
tf.config.experimental.list_physical_devices(device_type=None)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

df = pd.read_excel('/content/drive/MyDrive/Participants_Data_News_category/Data_Train.xlsx')

df.head()

df.isna().sum().sum()

sum(df.STORY.value_counts() > 1)

pd.DataFrame(df.STORY.value_counts())

df = df.drop_duplicates(subset=['STORY'])

sum(df.STORY.value_counts() > 1)

pd.DataFrame(df.STORY.value_counts())

df.drop(df[df.STORY == 'This story has been published from a wire agency feed without modifications to the text. Only the headline has been changed.'].index,inplace=True,axis = 0)

df[df.STORY=='This story has been published from a wire agency feed without modifications to the text. Only the headline has been changed.']

import re
import nltk
from string import punctuation
from collections import Counter
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('words')

stopwords = nltk.corpus.stopwords.words('english')
#stopwords.extend(['said','india','indian','two','year', 'market', 'price', 'u', 'company', 'also', 'last', 'new', 'may', 'time', 'data', 'million', 'government', 'like', 'one', 'first', 'would', 'people', 'say', 'make', 'screen', 'come'])
stopwords_dict = Counter(stopwords)
lmtzr = WordNetLemmatizer()
words = set(nltk.corpus.words.words())

"""## Data preprocessing for word embeddings"""

'''#data cleaning
def clean_text(text):    
    """
    text cleaning using regex
    """
    try:
        text=re.sub(r'^https?:\/\/<>.*[\r\n]*','',text,flags=re.MULTILINE)
        text=re.sub(r"[^A-Za-z]"," ",text)
        words=text.strip().lower().split()
        words=[w for w in words if len(w) >= 1]
        return " ".join(words)
    except:
        return ""'''

# Text cleaning 
import re, string
def clean_text(text):
    text=re.sub(r'^https?:\/\/<>.*[\r\n]*','',text,flags=re.MULTILINE)
    text=re.sub("[^a-zA-Z]"," ",str(text))
    text=text.lower()
    return re.sub("^\d+\s|\s\d+\s|\s\d+$", " ", text)

def lemmatize_sentences(sentence):
    tokens = sentence.split()
    lemmatized_tokens = [lmtzr.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized_tokens)

#apply the text cleaning function on the summary column  
df['news']=df.STORY.apply(clean_text)
df['news'] = df['news'].apply(lemmatize_sentences)

df.head()

"""# Generating word embeddings"""

import gensim

# convert each review into a word list 
corpus=[]
#iterate over each review 
for review in df['news']:
    corpus.append(review.split(" "))

len(corpus)

#build the word2vec model
model=gensim.models.Word2Vec(corpus,                                            #individual words for each review 
                            min_count=5,                                        #ignore words with frequency/occurance less than 10
                            size=150,                                           #word2vec embedding size 
                            window=6,                                           #neighbouring words to predict
                            iter=25,                                            #number of iterations 
                            #sg = 1 
                            )

model.wv.syn0.shape

weights_1 = model.wv.syn0

len(model.wv.vocab)

## Most similar words 
model.wv.most_similar('share')

model.save("word2vec.model")

model['modi'] + model['india']

"""## TEXT DATA PREPROCESSING"""

stopwords.extend(['year', 'said', 'also', 'last', 'new', 'time', 'two','would'])
stopwords_dict = Counter(stopwords)

def cleaning_text(text):
    remove_terms = punctuation + '0123456789'
    
    text = text.apply(lambda x: ' '.join([word for word in x.split() if word not in remove_terms]))
    text = text.apply(lambda x: x.strip())
    text = text.str.replace(' +',' ')
    text = text.apply(lambda x: ' '.join([word for word in x.split() if word[0]!='@']))
    text = text.apply(lambda x: ' '.join([word for word in x.split() if word[0:4]!='http']))
    text = text.apply(lambda x: re.sub('[^A-Za-z]+', ' ', x)) 
    text = text.apply(lambda x: re.sub(r'http\S+', '', x))
    text = text.apply(lambda x: x.lower())
    text = text.apply(lambda x: x.rstrip())
    text = text.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords_dict]))
    #text = text.apply(lambda x: ' '.join([word for word in nltk.wordpunct_tokenize(x) if word.lower() in words or not word.isalpha()]))
    
    return text

df['new_story'] = cleaning_text(df['STORY'])

df.head()

df.isna().sum().sum()

from collections import Counter
most_common = []
for text in df["new_story"].values:
    for word in text.split():
        most_common.append(word)

Counter(most_common).most_common(10)

from collections import Counter
least_common = []
for text in df["new_story"].values:
    for word in text.split():
        least_common.append(word)

Counter(least_common).most_common()[-50:-1]

df['new_story'] = df['new_story'].apply(lemmatize_sentences)

df.isna().sum().sum()

df.head()

"""# Visualizing word cloud for different Sections"""

most_common_0 = []
for text in df[df["SECTION"] == 0]['new_story'].values:
    for word in text.split():
        most_common_0.append(word)

class_0 = Counter(most_common_0).most_common(50)

most_common_1 = []
for text in df[df["SECTION"] == 1]['new_story'].values:
    for word in text.split():
        most_common_1.append(word)

class_1 = Counter(most_common_1).most_common(50)

most_common_2 = []
for text in df[df["SECTION"] == 2]['new_story'].values:
    for word in text.split():
        most_common_2.append(word)

class_2 = Counter(most_common_2).most_common(50)

most_common_3 = []
for text in df[df["SECTION"] == 3]['new_story'].values:
    for word in text.split():
        most_common_3.append(word)

class_3 = Counter(most_common_3).most_common(50)

#sorted(class_3,key=lambda class_3: class_3[1],reverse=True)

list_1 = []
for i in list(class_3+class_2+class_1+class_0):
    list_1.append(i[0])

print([item for item, count in Counter(list_1).items() if count > 3])

# importing necessary libraries for creating word cloud
from wordcloud import WordCloud,STOPWORDS

# function to create a wordcloud
def wordcloud_draw(data, color = 'black'):
    words = ' '.join(data)
    cleaned_word = " ".join([word for word in words.split()])
    wordcloud = WordCloud(stopwords=STOPWORDS,
                      background_color=color,
                      width=2500,
                      height=2000
                     ).generate(cleaned_word)
    plt.figure(1,figsize=(15, 15))
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.show()

df.SECTION.value_counts()

## Visualizing Positive Sentiments reviews:
print("Word Cloud of SECTION = 0")
wordcloud_draw(df[df.SECTION == 0]['new_story'])

## Visualizing Positive Sentiments reviews:
print("Word Cloud of SECTION = 1")
wordcloud_draw(df[df.SECTION == 1]['new_story'])

## Visualizing Positive Sentiments reviews:
print("Word Cloud of SECTION = 2")
wordcloud_draw(df[df.SECTION == 2]['new_story'])

## Visualizing Positive Sentiments reviews:
print("Word Cloud of SECTION = 3")
wordcloud_draw(df[df.SECTION == 3]['new_story'])

"""# Tokenizing"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding,Dropout,LSTM, Input, InputLayer, Dense, Bidirectional
from tensorflow.keras.models import Model, Sequential
from keras.preprocessing.sequence import pad_sequences
import tensorflow as tf

news_data = df.new_story

# Pre processing - reshape the data and preparing to train
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.new_story, df.SECTION, test_size=0.1,random_state=42)

t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')
# fit the tokenizer on the documents
t.fit_on_texts(news_data)
t.word_index['<PAD>'] = 0

max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']

train_sequences = t.texts_to_sequences(X_train)
test_sequences = t.texts_to_sequences(X_test)

print("Vocabulary size={}".format(len(t.word_index)))
print("Number of Documents={}".format(t.document_count))

train_lens = [len(s) for s in train_sequences]
test_lens = [len(s) for s in test_sequences]

fig, ax = plt.subplots(1,2, figsize=(12, 6))
h1 = ax[0].hist(train_lens)
h2 = ax[1].hist(test_lens)

MAX_SEQUENCE_LENGTH = 200

# pad dataset to a maximum review length in words
X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')
X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH,padding='post')
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train.shape, X_test.shape

y_train = np.array(pd.get_dummies(y_train))
y_test = np.array(pd.get_dummies(y_test))

y_train.shape, y_test.shape

"""# If we want use the pre-trained embeddings"""

# Load Pre-trained Word embeddings
# Update file location of glove.6B.100d.txt according to your working environment

embedding_path = '/content/drive/MyDrive/GL_hackathon/glove.twitter.27B.200d.txt'

embeddings_index = dict()
f = open(embedding_path)
for line in f:
	values = line.split()
	word = values[0]
	coefs = np.asarray(values[1:], dtype='float32')
	embeddings_index[word] = coefs
f.close()

# Create Word Embedding Matrix
embedding_matrix = np.zeros((max_vocab, 100))
for i in range(1,max_vocab):
	embedding_vector = embeddings_index.get(t.index_word[i])
	if embedding_vector is not None:
		embedding_matrix[i] = embedding_vector

"""# end of using pre-trained embeddings"""

# Define Vocabulary size (no. of most frequent tokens) to consider
max_vocab = len(t.word_index)
max_vocab

t.index_word[1]

# Create Word Embedding Matrix
embedding_matrix = np.zeros((max_vocab, 150))
for i in range(1,max_vocab):
    try:
    	embedding_vector = model[t.index_word[i]]
    	if embedding_vector is not None:
	    	embedding_matrix[i] = embedding_vector
    ## if word not in model then skip and the row stays all 0s
    except:
        pass

embedding_matrix.shape

t.index_word[3]

embedding_matrix[3]

EMBEDDING_DIM = 150 # dimension for dense embeddings for each token
LSTM_DIM = 128 # total LSTM units

model = tf.keras.models.Sequential()
model.add(Embedding(input_dim=max_vocab, 
                    output_dim=EMBEDDING_DIM, 
                    input_length=MAX_SEQUENCE_LENGTH,
                    weights=[embedding_matrix],
                    name='own_embedding',
                    trainable=False))
#model.add(tf.keras.layers.SpatialDropout1D(0.1))
model.add(Bidirectional(LSTM(LSTM_DIM,return_sequences=False)))
#model.add(LSTM(LSTM_DIM))
model.add(tf.keras.layers.Dense(256, activation="relu"))
model.add(tf.keras.layers.Dense(4, activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam",
              metrics=["accuracy"])
model.summary()

batch_size = 128
model.fit(X_train, y_train, epochs=60, 
          batch_size=batch_size, 
          shuffle=True, 
          validation_data=(X_test,y_test), 
          verbose=2, workers = 4)

# MLP for Pima Indians Dataset Serialize to JSON and HDF5
#from keras.models import Sequential
#from keras.layers import Dense
from keras.models import model_from_json
import numpy
import os

# serialize model to JSON
model_json = model.to_json()
with open("/content/model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("/content/model.h5")
print("Saved model to disk")
 
# later...
 
# load json and create model
json_file = open('/content/model.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
loaded_model = model_from_json(loaded_model_json)
# load weights into new model
loaded_model.load_weights("/content/model.h5")
print("Loaded model from disk")

"""# ---> Prediction on test data"""

test = pd.read_excel('/content/drive/MyDrive/Participants_Data_News_category/Data_Test.xlsx')

test['new_story'] = cleaning_text(test['STORY'])

test.head()

test['new_story'] = test['new_story'].apply(lemmatize_sentences)

news_test_data = test.new_story

test_data = t.texts_to_sequences(news_test_data)

test_data = tf.keras.preprocessing.sequence.pad_sequences(test_data, maxlen=MAX_SEQUENCE_LENGTH,padding='post')

test_data.shape

prediction = model.predict_classes(test_data)

pred = loaded_model.predict_classes(test_data)

pd.DataFrame(prediction,pred)

pd.DataFrame(prediction,columns=['SECTION']).to_excel('sub_14.xlsx',index=False)

